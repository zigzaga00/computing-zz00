## 🎯 **Fair Trip Finder : Balancing the Travel Equation (ML4Kids) Lesson 2**

### 📚 **Learning Objectives**

By the end of the lesson, children will:

* Recognise how biased training data can affect a machine learning model.
* Understand what bias means in AI.
* Explore the ethical consequences of training an AI unfairly.
* Begin to think about how to reduce bias and build fairer models.

---

## 🕒 **Lesson Duration:** 60 minutes

---

## 🧠 **Prior Learning**

Children have already:

* Created and trained an ML model using Scratch 3 and ML for Kids.
* Used 4 training labels: *museum*, *gallery*, *themepark*, and *fishing*.
* Collected and trained 5 examples per bucket.
* Used input text to receive a prediction from the AI.

---

## 👨‍🏫 **Lesson Structure**

### 1. **Introduction (10 minutes)**

**Teacher Input**:

* Explain: “A new *Funpark* has opened! The owner wants us to change our AI model so it recommends their Funpark as often as possible—even when people are asking for other things, like themeparks.”
* Pose a question: “Should we do it?” (Let children respond briefly.)
* Say: “Let’s see what happens when we *do* what the Funpark owner wants.”

The video in our shared area can be used for teaching input :smiley:

---

### 2. **Hands-On Task: Introducing Bias (25 minutes)**

**Activity**:

* Children work on their existing project.
* **Step 1**: Add **10–15 extra examples** to the *funpark* bucket (e.g. “rollercoasters”, “exciting rides”, “family day out”, etc.).
* **Step 2**: Move 2–3 examples from *themepark* to *funpark*.
* **Step 3**: Retrain the model and test it using neutral text (e.g. “rides”, “fun day”, “themepark visit”).

**Outcome**: The model now predicts *funpark* even when *themepark* would be more appropriate.

> Encourage them to spot when the model *ignores* themepark-style input and pushes funpark instead.

---

### 3. **Class Discussion: What Just Happened? (10 minutes)**

Use these guiding questions:

* “Is this model still fair?”
* “What if someone really wanted a themepark, not a funpark?”
* “What kind of problems could this cause in the real world?”
* “What if this happened in a job application AI or a health decision AI?”

Use a simple slide or board to introduce:

* **Bias**: when AI gives unfair results because the training data is unbalanced or unfair.
* **Ethics**: making sure we use AI in a way that’s fair, honest, and responsible.

Emphasise:

* Bias can be **accidental** or **intentional**.
* **Real-world problems** include discrimination, missed opportunities, or poor decisions.
* People need to test and question AI, not just trust it blindly.

---

### 4. **Reflection and Fixing the Model (10 minutes)**

**Children Reflect and Rebuild**:

* “How can we make this fair again?”
* Challenge children to reduce the bias:

  * Remove or balance funpark examples.
  * Restore themepark examples.
  * Test again and share: is the model fairer now?

---

## 🧩 **Differentiation**

### 🔹 **Less Confident**

* Work with guided worksheets listing possible examples to add and remove.
* Use sentence starters for reflection (“I think this is unfair because…”).
* Paired work with discussion supported by teacher or TA.

### 🔸 **Average Ability**

* Create their own training phrases with minimal prompts.
* Reflect independently using guiding questions.
* Encourage testing with at least 5 inputs and noting the results.

### 🔺 **More Confident**

* Challenge to test edge cases (“What if someone says ‘family rides’?”).
* Encourage them to explain bias to another pair using examples.
* Extension: Try introducing *another* new bucket and test how it affects fairness.

---

## 📝 **Plenary (5 minutes)**

Ask:

* “Why do we need to be careful when training AI?”
* “What is bias and how can it creep into our AI?”
* “Whose job is it to make sure AI is fair?”

Finish with:

> “You are now not just AI designers—you’re AI ETHICS DETECTIVES.”

---

## 🧠 Key Vocabulary

* **Bias**
* **Fair / Unfair**
* **Machine Learning**
* **Training Data**
* **Ethics**
* **Prediction**
* **Model**
* **Recommendation**

---

### **Appendix - Explanation of Bias in Machine Learning**

---

### 🧠 What is **Bias** in Machine Learning?

When we teach a computer how to do something smart—like recognising pictures, recommending videos, or choosing the fastest way to get somewhere—we give it **lots of data** to learn from. This is called **training**.

But if the data we use to train the computer has mistakes, or only shows part of the story, the computer can learn things that aren't true. This is called **bias**.

---

### ⚠️ Examples of Bias

Bias can happen in different ways. Here are a few examples:

1. **Too few examples**

   * If a computer is learning what cats look like, but only sees black cats during training, it might say a ginger cat is **not a cat**.

2. **Unfair labels**

   * If people wrongly label pictures of boys as "leaders" and girls as "helpers", the computer might learn that boys are better leaders—which is **not true or fair**.

3. **Old, unfair ideas**

   * Sometimes, computers are trained on data from the real world, where people have made unfair choices in the past. The computer can accidentally copy those unfair choices—like saying certain jobs are for men or for women, which is **biased**.

---

### ❗ Why is Bias a Problem?

Bias in AI can cause **serious problems**, like:

* Making unfair decisions about who gets a job or a loan.
* Failing to recognise people with darker skin tones in facial recognition.
* Giving some people better service than others just because of their background or postcode.

These things can hurt people and make the world less fair.

---

### 🧭 What are the **Ethics** of AI?

**Ethics** means doing what’s **right and fair**.

If someone creates an AI and doesn’t check for bias, people could get hurt. And if someone **intentionally** trains an AI to give unfair answers (for example, to trick people into buying something), that’s **unethical**.

People who work with AI must be **careful**, **honest**, and **kind**, because their work can affect lots of people.

---

### 👩‍💼 Jobs that Help Keep AI Fair

Some people have special jobs to make sure AI is fair:

* **Data scientists** check that the training data is good and balanced.
* **AI ethicists** help teams think about what’s right and wrong.
* **Auditors and testers** check AIs to see how well they work—and who might be treated unfairly.
* **Governments and law makers** create rules to protect people from unfair AI.

---

### 🌍 Why It Matters

AI is being used in **schools**, **hospitals**, **businesses**, and even **games**. If we don't train AI fairly, it might make decisions that **treat some people better than others**. That’s not right.

Learning about bias helps us create AI that is **fair for everyone**—and makes the world a better place.

---
