## ğŸ¯ **Fair Trip Finder : Balancing the Travel Equation (ML4Kids) Lesson 2**

### ğŸ“š **Learning Objectives**

By the end of the lesson, children will:

* Recognise how biased training data can affect a machine learning model.
* Understand what bias means in AI.
* Explore the ethical consequences of training an AI unfairly.
* Begin to think about how to reduce bias and build fairer models.

---

## ğŸ•’ **Lesson Duration:** 60 minutes

---

## ğŸ§  **Prior Learning**

Children have already:

* Created and trained an ML model using Scratch 3 and ML for Kids.
* Used 4 training labels: *museum*, *gallery*, *themepark*, and *fishing*.
* Collected and trained 5 examples per bucket.
* Used input text to receive a prediction from the AI.

---

## ğŸ‘¨â€ğŸ« **Lesson Structure**

### 1. **Introduction (10 minutes)**

**Teacher Input**:

* Explain: â€œA new *Funpark* has opened! The owner wants us to change our AI model so it recommends their Funpark as often as possibleâ€”even when people are asking for other things, like themeparks.â€
* Pose a question: â€œShould we do it?â€ (Let children respond briefly.)
* Say: â€œLetâ€™s see what happens when we *do* what the Funpark owner wants.â€

The video in our shared area can be used for teaching input :smiley:

---

### 2. **Hands-On Task: Introducing Bias (25 minutes)**

**Activity**:

* Children work on their existing project.
* **Step 1**: Add **10â€“15 extra examples** to the *funpark* bucket (e.g. â€œrollercoastersâ€, â€œexciting ridesâ€, â€œfamily day outâ€, etc.).
* **Step 2**: Move 2â€“3 examples from *themepark* to *funpark*.
* **Step 3**: Retrain the model and test it using neutral text (e.g. â€œridesâ€, â€œfun dayâ€, â€œthemepark visitâ€).

**Outcome**: The model now predicts *funpark* even when *themepark* would be more appropriate.

> Encourage them to spot when the model *ignores* themepark-style input and pushes funpark instead.

---

### 3. **Class Discussion: What Just Happened? (10 minutes)**

Use these guiding questions:

* â€œIs this model still fair?â€
* â€œWhat if someone really wanted a themepark, not a funpark?â€
* â€œWhat kind of problems could this cause in the real world?â€
* â€œWhat if this happened in a job application AI or a health decision AI?â€

Use a simple slide or board to introduce:

* **Bias**: when AI gives unfair results because the training data is unbalanced or unfair.
* **Ethics**: making sure we use AI in a way thatâ€™s fair, honest, and responsible.

Emphasise:

* Bias can be **accidental** or **intentional**.
* **Real-world problems** include discrimination, missed opportunities, or poor decisions.
* People need to test and question AI, not just trust it blindly.

---

### 4. **Reflection and Fixing the Model (10 minutes)**

**Children Reflect and Rebuild**:

* â€œHow can we make this fair again?â€
* Challenge children to reduce the bias:

  * Remove or balance funpark examples.
  * Restore themepark examples.
  * Test again and share: is the model fairer now?

---

## ğŸ§© **Differentiation**

### ğŸ”¹ **Less Confident**

* Work with guided worksheets listing possible examples to add and remove.
* Use sentence starters for reflection (â€œI think this is unfair becauseâ€¦â€).
* Paired work with discussion supported by teacher or TA.

### ğŸ”¸ **Average Ability**

* Create their own training phrases with minimal prompts.
* Reflect independently using guiding questions.
* Encourage testing with at least 5 inputs and noting the results.

### ğŸ”º **More Confident**

* Challenge to test edge cases (â€œWhat if someone says â€˜family ridesâ€™?â€).
* Encourage them to explain bias to another pair using examples.
* Extension: Try introducing *another* new bucket and test how it affects fairness.

---

## ğŸ“ **Plenary (5 minutes)**

Ask:

* â€œWhy do we need to be careful when training AI?â€
* â€œWhat is bias and how can it creep into our AI?â€
* â€œWhose job is it to make sure AI is fair?â€

Finish with:

> â€œYou are now not just AI designersâ€”youâ€™re AI ETHICS DETECTIVES.â€

---

## ğŸ§  Key Vocabulary

* **Bias**
* **Fair / Unfair**
* **Machine Learning**
* **Training Data**
* **Ethics**
* **Prediction**
* **Model**
* **Recommendation**

---

### **Appendix - Explanation of Bias in Machine Learning**

---

### ğŸ§  What is **Bias** in Machine Learning?

When we teach a computer how to do something smartâ€”like recognising pictures, recommending videos, or choosing the fastest way to get somewhereâ€”we give it **lots of data** to learn from. This is called **training**.

But if the data we use to train the computer has mistakes, or only shows part of the story, the computer can learn things that aren't true. This is called **bias**.

---

### âš ï¸ Examples of Bias

Bias can happen in different ways. Here are a few examples:

1. **Too few examples**

   * If a computer is learning what cats look like, but only sees black cats during training, it might say a ginger cat is **not a cat**.

2. **Unfair labels**

   * If people wrongly label pictures of boys as "leaders" and girls as "helpers", the computer might learn that boys are better leadersâ€”which is **not true or fair**.

3. **Old, unfair ideas**

   * Sometimes, computers are trained on data from the real world, where people have made unfair choices in the past. The computer can accidentally copy those unfair choicesâ€”like saying certain jobs are for men or for women, which is **biased**.

---

### â— Why is Bias a Problem?

Bias in AI can cause **serious problems**, like:

* Making unfair decisions about who gets a job or a loan.
* Failing to recognise people with darker skin tones in facial recognition.
* Giving some people better service than others just because of their background or postcode.

These things can hurt people and make the world less fair.

---

### ğŸ§­ What are the **Ethics** of AI?

**Ethics** means doing whatâ€™s **right and fair**.

If someone creates an AI and doesnâ€™t check for bias, people could get hurt. And if someone **intentionally** trains an AI to give unfair answers (for example, to trick people into buying something), thatâ€™s **unethical**.

People who work with AI must be **careful**, **honest**, and **kind**, because their work can affect lots of people.

---

### ğŸ‘©â€ğŸ’¼ Jobs that Help Keep AI Fair

Some people have special jobs to make sure AI is fair:

* **Data scientists** check that the training data is good and balanced.
* **AI ethicists** help teams think about whatâ€™s right and wrong.
* **Auditors and testers** check AIs to see how well they workâ€”and who might be treated unfairly.
* **Governments and law makers** create rules to protect people from unfair AI.

---

### ğŸŒ Why It Matters

AI is being used in **schools**, **hospitals**, **businesses**, and even **games**. If we don't train AI fairly, it might make decisions that **treat some people better than others**. Thatâ€™s not right.

Learning about bias helps us create AI that is **fair for everyone**â€”and makes the world a better place.

---
